{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import random\n",
      "from numpy import sqrt\n",
      "from activation_functions import *\n",
      "from cost_functions import *\n",
      "from numpy.linalg import norm\n",
      "\n",
      "class NeuralNetwork:\n",
      "    def __init__(self, X, y, layers, alpha=0.1, test_prop=0.9, seed_parameter=1, activation_function='logistic', loss_function='squared_loss', output_function='linear'):\n",
      "\n",
      "\tself.X = X\n",
      "\tself.y = y\n",
      "\tself.N, self.P = X.shape\n",
      "\n",
      "\tself.seed = seed_parameter\n",
      "\tself.layers = layers\n",
      "\t\n",
      "\tself.N_train = np.floor(self.N*(1-test_prop))\n",
      "\tself.N_test = self.N - self.N_train\n",
      "\n",
      "\trow_idx = [i for i in xrange(self.N)]\n",
      "\trandom.shuffle(row_idx)\n",
      "\t'''\n",
      "\tself.X_train = self.X[row_idx[:self.N_train],:]\n",
      "\tself.X_test = self.X[row_idx[self.N_train:],:]\n",
      "\n",
      "\tself.y_train = self.y[row_idx[:self.N_train],:]\n",
      "\tself.y_test = self.y[row_idx[self.N_train:],:]\n",
      "\t'''\t\n",
      "\n",
      "\tself.W = {} # weights\n",
      "\tself.b = {} # biases\n",
      "\tself.alpha = alpha # learning rate\n",
      "\n",
      "\tself.activation_function, self.activation_gradient = get_activation_function(activation_function)\n",
      "\tself.loss_function = get_cost_function(loss_function)\n",
      "\tself.output_function, self.output_gradient = get_activation_function(output_function)\n",
      "\n",
      "    def normalization(X):\n",
      "\tP = X.shape[1]\n",
      "\tfor p in xrange(P):\t\t\n",
      "\t    mean= np.mean(X[:,p])\n",
      "\t    variance = np.variance(X[:,p])\n",
      "\t    X[:,p]= (X[:,p] - mean)/variance \n",
      "\treturn X\t\n",
      "\n",
      "\n",
      "    def w_initial(self, input, output):\n",
      "\treturn sqrt(6.0/(input+output)) \n",
      "\n",
      "    def initialize_weights(self):\n",
      "        inputs = self.P\n",
      "        outputs = self.layers[0]\n",
      "        w = self.w_initial(inputs, outputs)\n",
      "        self.W[0] = np.random.uniform(-w, w, size=(inputs, outputs))\n",
      "        self.b[0] = 0\t\n",
      "\n",
      "        for i in xrange(1, len(self.layers)):\n",
      "            inputs = self.layers[i-1]\n",
      "            outputs = self.layers[i] \n",
      "            w = self.w_initial(inputs, outputs)\n",
      "            self.W[i] = np.random.uniform(-w, w, size=(inputs, outputs))\n",
      "            self.b[i] = 0\n",
      "\n",
      "    def compute_loss(self, X, y):\n",
      "        return self.loss_function(X,y)\n",
      "\n",
      "    def compute_error(self, X, y):\n",
      "        return y-X\n",
      "    \n",
      "    \n",
      "    def feed_forward(self, X):\n",
      "        N = X.shape[0]\n",
      "        self.W_length = len(self.W)\t \n",
      "        z = {}\n",
      "        z_new = X\n",
      "\n",
      "        for layer_idx in xrange(self.W_length):\n",
      "            z[layer_idx] = z_new\n",
      "            P = self.W[layer_idx].shape[1]\n",
      "            z_new = np.zeros((N,P))\n",
      "            for p in xrange(P):\n",
      "                z_new[:, p] = self.activation_function(z[layer_idx].dot(self.W[layer_idx][:,p]) + np.ones(N)*self.b[layer_idx])\n",
      "        #outer layer\n",
      "        P = self.W[self.W_length-1].shape[1]\n",
      "        for p in xrange(P):\n",
      "            #z_new[:, p] = self.output_function(z[layer_idx], self.W[self.W_length-1][:,p], self.b[self.W_length-1])\n",
      "            z_new[:, p] = z[layer_idx].dot(self.W[self.W_length-1][:,p]) + self.b[self.W_length-1]\n",
      "        z[self.W_length] = z_new\n",
      "        return z\t\n",
      "\n",
      "\n",
      "    def back_propagation(self, X, y, z):\n",
      "        \n",
      "        batch_error = self.compute_error(z[self.W_length], y)\n",
      "        \n",
      "\t# update step for output layer (linear update)\n",
      "        batch_update = np.zeros(self.W[self.W_length-1].shape)\n",
      "        for i,e in enumerate(batch_error):\n",
      "            delta = (e*z[self.W_length-1][i,:])\n",
      "            #print self.W[self.W_length-1].shape\n",
      "            #print np.array([delta]).shape\n",
      "            self.W[self.W_length-1] -= self.alpha*np.array([delta]).T\n",
      "\n",
      "    # backprop for inside layers\n",
      "            for layer_idx in xrange(self.W_length-2, 0, -1):\n",
      "                #delta_old = delta\n",
      "                #delta = self.activation_gradient(z[layer_idx].dot(self.W[layer_idx]))*delta_old.dot(self.W[layer_idx+1])\n",
      "                #self.W[layer_idx] -= self.alpha*delta.dot(self.W[layer_idx-1].dot(z[layer_idx-1])\t)\n",
      "                len_z = len(z[layer_idx][i,:]) \n",
      "                len_W_col=len(z[layer_idx][i,:].dot(self.W[layer_idx]))\n",
      "                #print 'W: ', self.W[layer_idx]\n",
      "                self.W[layer_idx] -= self.alpha*e*z[layer_idx][i,:].reshape((len_z,1)).dot(self.activation_gradient(z[layer_idx][i,:].dot(self.W[0])).reshape((1,len_W_col)))\n",
      "            #delta_old = delta\n",
      "            #delta = self.activation_gradient(z[0][i,:].dot(self.W[0]))*(delta_old*self.W[1]).T\n",
      "            #self.W[0] -= self.alpha*(delta.T*z[0][i,:]).T\n",
      "            #print 'z: ', z[0][i,:].shape\n",
      "            #print 'W: ', self.W[0]\n",
      "            #print 'activation: ', self.activation_gradient(z[0][i,:].dot(self.W[0])).shape\n",
      "            len_W_col=len(z[0][i,:].dot(self.W[0]))\n",
      "            len_z = len(z[0][i,:])\n",
      "            self.W[0] -= self.alpha*e*z[0][i,:].reshape((len_z,1)).dot(self.activation_gradient(z[0][i,:].dot(self.W[0])).reshape((1,len_W_col)))\n",
      "                                                                           \n",
      "    def main(self):\n",
      "        self.initialize_weights()\n",
      "        for i in xrange(25):\n",
      "            z = self.feed_forward(self.X)\n",
      "            print 0.5*norm(z[self.W_length]-y)**2\n",
      "            self.back_propagation(X, y, z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "N = 10\n",
      "P = 3\n",
      "layers = [4, 1]\n",
      "X = np.random.uniform(0, 1, size=(N,P))\n",
      "y = np.random.uniform(0, 1, size=(N,1))\n",
      "\n",
      "nn = NeuralNetwork(X, y, layers)\n",
      "nn.main()\n",
      "#nn.initialize_weights()\n",
      "#'''\n",
      "#nn.main()\n",
      "#nn.initialize_weights()\n",
      "#z = nn.feed_forward(X)\n",
      "'''\n",
      "nn.back_propagation(X,y,z)\n",
      "print nn.W\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.38968513892\n",
        "10.6508967909\n",
        "39.9801337202\n",
        "92.7347279482\n",
        "119.246015579\n",
        "121.635620185\n",
        "119.991653323\n",
        "117.305939899\n",
        "114.026633312\n",
        "110.385223374\n",
        "106.561647192\n",
        "102.691367491\n",
        "98.8694754318\n",
        "95.1584034139"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "91.5961673748\n",
        "88.2034503418\n",
        "84.9890933599\n",
        "81.9541162082\n",
        "79.0945597726\n",
        "76.4034472063"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "73.8721113025\n",
        "71.4910768258\n",
        "69.250635273\n",
        "67.1412095454\n",
        "65.153576535\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "'\\nnn.back_propagation(X,y,z)\\nprint nn.W\\n'"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = nn.feed_forward(X)\n",
      "nn.back_propagation(X,y,z)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "z:  (3,)\n",
        "W:  [[-0.1382298  -0.44260637 -0.36642054 -0.8362439 ]\n",
        " [-0.01131553  0.61419844  0.41739628 -0.48790822]\n",
        " [ 0.26853899  0.45162448  0.52493354 -0.14244592]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[-0.10576984 -0.4113182  -0.33498493 -0.81170855]\n",
        " [ 0.01735318  0.64183223  0.44516029 -0.46623854]\n",
        " [ 0.29747911  0.47951987  0.55296039 -0.1205711 ]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[-0.1057016  -0.41125824 -0.33492218 -0.81164341]\n",
        " [ 0.04366134  0.66494744  0.46935342 -0.44112118]\n",
        " [ 0.30923725  0.48985097  0.56377325 -0.10934517]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[-0.07968246 -0.38662846 -0.31029942 -0.79129627]\n",
        " [ 0.06414803  0.68434021  0.48874065 -0.42510045]\n",
        " [ 0.33706459  0.5161924   0.59010716 -0.08758401]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[-0.05924717 -0.36770431 -0.29109939 -0.77597631]\n",
        " [ 0.08420485  0.70291386  0.50758507 -0.41006423]\n",
        " [ 0.35269051  0.53066282  0.60478853 -0.07586955]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[-0.04442288 -0.3544609  -0.27746969 -0.76364061]\n",
        " [ 0.1058901   0.72228657  0.52752286 -0.39201933]\n",
        " [ 0.36643744  0.54294375  0.61742768 -0.06443035]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[-0.02617836 -0.33687945 -0.25992482 -0.74676039]\n",
        " [ 0.11999419  0.73587807  0.54108608 -0.37896992]\n",
        " [ 0.38852933  0.56423275  0.63867239 -0.04399045]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[ 0.00876212 -0.30194939 -0.22522134 -0.71582831]\n",
        " [ 0.13123573  0.74711626  0.55225137 -0.36901802]\n",
        " [ 0.41155937  0.58725593  0.66154622 -0.02360244]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[ 0.01254269 -0.29873209 -0.22184626 -0.71210577]\n",
        " [ 0.16702283  0.77757139  0.58420004 -0.3337803 ]\n",
        " [ 0.43043972  0.60332327  0.67840152 -0.00501193]]\n",
        "activation:  (4,)\n",
        "z:  (3,)\n",
        "W:  [[ 0.01336811 -0.29795696 -0.22105145 -0.71128618]\n",
        " [ 0.18722079  0.79653891  0.60364897 -0.31372483]\n",
        " [ 0.43365195  0.60633981  0.68149462 -0.00182236]]\n",
        "activation:  (4,)\n"
       ]
      }
     ],
     "prompt_number": 49
    }
   ],
   "metadata": {}
  }
 ]
}