{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import random\n",
      "from numpy import sqrt\n",
      "from activation_functions import *\n",
      "from cost_functions import *\n",
      "from numpy.linalg import norm\n",
      "\n",
      "class NeuralNetwork:\n",
      "    def __init__(self, X, y, layers, alpha=0.1, test_prop=0.9, seed_parameter=1, activation_function='logistic', loss_function='squared_loss', output_function='linear'):\n",
      "\n",
      "\tself.X = X\n",
      "\tself.y = y\n",
      "\tself.N, self.P = X.shape\n",
      "\n",
      "\tself.seed = seed_parameter\n",
      "\tself.layers = layers\n",
      "\t\n",
      "\tself.N_train = np.floor(self.N*(1-test_prop))\n",
      "\tself.N_test = self.N - self.N_train\n",
      "\n",
      "\trow_idx = [i for i in xrange(self.N)]\n",
      "\trandom.shuffle(row_idx)\n",
      "\t'''\n",
      "\tself.X_train = self.X[row_idx[:self.N_train],:]\n",
      "\tself.X_test = self.X[row_idx[self.N_train:],:]\n",
      "\n",
      "\tself.y_train = self.y[row_idx[:self.N_train],:]\n",
      "\tself.y_test = self.y[row_idx[self.N_train:],:]\n",
      "\t'''\t\n",
      "\n",
      "\tself.W = {} # weights\n",
      "\tself.b = {} # biases\n",
      "\tself.alpha = alpha # learning rate\n",
      "\n",
      "\tself.activation_function, self.activation_gradient = get_activation_function(activation_function)\n",
      "\tself.loss_function = get_cost_function(loss_function)\n",
      "\tself.output_function, self.output_gradient = get_activation_function(output_function)\n",
      "\n",
      "    def normalization(X):\n",
      "\tP = X.shape[1]\n",
      "\tfor p in xrange(P):\t\t\n",
      "\t    mean= np.mean(X[:,p])\n",
      "\t    variance = np.variance(X[:,p])\n",
      "\t    X[:,p]= (X[:,p] - mean)/variance \n",
      "\treturn X\t\n",
      "\n",
      "\n",
      "    def w_initial(self, input, output):\n",
      "\treturn sqrt(6.0/(input+output)) \n",
      "\n",
      "    def initialize_weights(self):\n",
      "        inputs = self.P\n",
      "        outputs = self.layers[0]\n",
      "        w = self.w_initial(inputs, outputs)\n",
      "        self.W[0] = np.random.uniform(-w, w, size=(inputs, outputs))\n",
      "        self.b[0] = 0\t\n",
      "\n",
      "        for i in xrange(1, len(self.layers)):\n",
      "            inputs = self.layers[i-1]\n",
      "            outputs = self.layers[i] \n",
      "            w = self.w_initial(inputs, outputs)\n",
      "            self.W[i] = np.random.uniform(-w, w, size=(inputs, outputs))\n",
      "            self.b[i] = 0\n",
      "\n",
      "    def compute_loss(self, X, y):\n",
      "        return self.loss_function(X,y)\n",
      "\n",
      "    def compute_error(self, X, y):\n",
      "        return y-X\n",
      "    \n",
      "    \n",
      "    def feed_forward(self, X):\n",
      "        N = X.shape[0]\n",
      "        self.W_length = len(self.W)\t \n",
      "        z = {}\n",
      "        z_new = X\n",
      "\n",
      "        for layer_idx in xrange(self.W_length):\n",
      "            z[layer_idx] = z_new\n",
      "            P = self.W[layer_idx].shape[1]\n",
      "            z_new = np.zeros((N,P))\n",
      "            for p in xrange(P):\n",
      "                z_new[:, p] = self.activation_function(z[layer_idx].dot(self.W[layer_idx][:,p]) + np.ones(N)*self.b[layer_idx])\n",
      "        #outer layer\n",
      "        P = self.W[self.W_length-1].shape[1]\n",
      "        for p in xrange(P):\n",
      "            #z_new[:, p] = self.output_function(z[layer_idx], self.W[self.W_length-1][:,p], self.b[self.W_length-1])\n",
      "            z_new[:, p] = z[layer_idx].dot(self.W[self.W_length-1][:,p]) + self.b[self.W_length-1]\n",
      "        z[self.W_length] = z_new\n",
      "        return z\t\n",
      "\n",
      "\n",
      "    def back_propagation(self, X, y, z):\t\n",
      "        #print self.W_length\n",
      "        batch_error = self.compute_error(z[self.W_length], y)\n",
      "        #print type(batch_error)\n",
      "        #print batch_error\n",
      "\t# update step for output layer\n",
      "        batch_update = np.zeros(self.W[self.W_length-1].shape)\n",
      "        for i,e in enumerate(batch_error):\n",
      "            delta = (e*self.W[self.W_length-1].T*z[self.W_length-1][i,:]).T\n",
      "            self.W[self.W_length-1] -= self.alpha*delta\n",
      "            #print 'W: ', self.W[self.W_length-1]\n",
      "\t# backprop for inside layers\n",
      "            for layer_idx in xrange(self.W_length-2, 0, -1):\n",
      "                #print 'layer index: ', layer_idx\n",
      "                delta_old = delta\n",
      "                delta = self.activation_gradient(z[layer_idx].dot(self.W[layer_idx]))*delta_old.dot(self.W[layer_idx+1])\n",
      "                self.W[layer_idx] -= self.alpha*delta.dot(self.W[layer_idx-1].dot(z[layer_idx-1])\t)\n",
      "            \n",
      "            delta_old = delta\t\n",
      "            #print delta_old.shape\n",
      "            #print self.W[0].shape\n",
      "            delta = self.activation_gradient(z[0][i,:].dot(self.W[0]))*(delta_old*self.W[1]).T\n",
      "            #print 'delta: ', delta\n",
      "            #print 'z[0]: ', z[0][i,:]\n",
      "            self.W[0] -= self.alpha*(delta.T*z[0][i,:]).T            \n",
      "            #print 'W[0]: ', self.W[0]\n",
      "            #print 'W[1]: ', self.W[1]\n",
      "            \n",
      "    def main(self):\n",
      "        self.initialize_weights()\n",
      "        for i in xrange(25):\n",
      "            z = self.feed_forward(self.X)\n",
      "            print 0.5*norm(z[self.W_length]-y)**2\n",
      "            self.back_propagation(X, y, z)\n",
      "\n",
      "        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "\n",
      "N = 10\n",
      "P = 3\n",
      "layers = [4, 1]\n",
      "\n",
      "X = np.random.uniform(0, 1, size=(N,P))\n",
      "y = np.random.uniform(0, 1, size=(N,1))\n",
      "\n",
      "nn = NeuralNetwork(X, y, layers)\n",
      "nn.main()\n",
      "#nn.initialize_weights()\n",
      "#z = nn.feed_forward(X)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.609014159824\n",
        "0.632603242332\n",
        "0.671873213787\n",
        "0.740043616836\n",
        "0.865436569774\n",
        "1.11606282895\n",
        "1.68473540783\n",
        "3.28417575543\n",
        "10.2551825111\n",
        "110.247745846\n",
        "104972.020489\n",
        "6.81633452966e+28\n",
        "1.51133178939e+290\n",
        "nan\n",
        "nan\n",
        "nan\n",
        "nan\n",
        "nan\n",
        "nan\n",
        "nan"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nan\n",
        "nan\n",
        "nan\n",
        "nan\n",
        "nan\n"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.back_propagation(X,y,z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "W[0]:  [[-0.79835983  0.51253642 -0.03842815 -0.78695599]\n",
        " [ 0.9200269  -0.33229876 -0.92568297  0.63088041]\n",
        " [ 0.23842811 -0.9068716   0.73311987  0.04548312]]\n",
        "W[1]:  [[-0.44293667]\n",
        " [-0.81821643]\n",
        " [ 0.80671587]\n",
        " [-0.1905165 ]]\n",
        "W[0]:  [[-0.80074102  0.50555298 -0.04357981 -0.78733802]\n",
        " [ 0.91774187 -0.33900016 -0.93062657  0.63051381]\n",
        " [ 0.2374273  -0.9098067   0.73095466  0.04532255]]\n",
        "W[1]:  [[-0.41670561]\n",
        " [-0.77694838]\n",
        " [ 0.77407617]\n",
        " [-0.18083313]]\n",
        "W[0]:  [[-0.80200278  0.50170972 -0.04672468 -0.78754895]\n",
        " [ 0.91653991 -0.3426613  -0.93362242  0.63031287]\n",
        " [ 0.23687121 -0.91150055  0.72956861  0.04522959]]\n",
        "W[1]:  [[-0.39883843]\n",
        " [-0.74786624]\n",
        " [ 0.74934812]\n",
        " [-0.17398956]]\n",
        "W[0]:  [[-0.80252776  0.50035965 -0.04824054 -0.78764263]\n",
        " [ 0.91559385 -0.34509422 -0.9363541   0.63014405]\n",
        " [ 0.23614085 -0.91337875  0.72745977  0.04509926]]\n",
        "W[1]:  [[-0.37616388]\n",
        " [-0.7170224 ]\n",
        " [ 0.71539679]\n",
        " [-0.16493762]]\n",
        "W[0]:  [[-0.80306624  0.49899199 -0.05057425 -0.78773226]\n",
        " [ 0.91539874 -0.34558976 -0.93719967  0.63011157]\n",
        " [ 0.23535877 -0.91536512  0.7240703   0.04496908]]\n",
        "W[1]:  [[-0.36653893]\n",
        " [-0.70319254]\n",
        " [ 0.69241328]\n",
        " [-0.16121723]]\n",
        "W[0]:  [[-0.80345048  0.49834258 -0.05174835 -0.7878025 ]\n",
        " [ 0.91429832 -0.3474496  -0.94056214  0.62991043]\n",
        " [ 0.23397039 -0.91771166  0.71982792  0.0447153 ]]\n",
        "W[1]:  [[-0.3441732 ]\n",
        " [-0.6824924 ]\n",
        " [ 0.65957423]\n",
        " [-0.15258761]]\n",
        "W[0]:  [[-0.80389227  0.49677901 -0.05305791 -0.78788279]\n",
        " [ 0.91385381 -0.34902283 -0.94187979  0.62982964]\n",
        " [ 0.23376404 -0.91844195  0.71921627  0.0446778 ]]\n",
        "W[1]:  [[-0.33231994]\n",
        " [-0.66138419]\n",
        " [ 0.6409994 ]\n",
        " [-0.14775045]]\n",
        "W[0]:  [[-0.80441437  0.49581471 -0.05465442 -0.78797991]\n",
        " [ 0.9126435  -0.3512582  -0.94558073  0.62960449]\n",
        " [ 0.23241942 -0.9209254   0.71510462  0.04442767]]\n",
        "W[1]:  [[-0.30821387]\n",
        " [-0.63812288]\n",
        " [ 0.60646279]\n",
        " [-0.1384446 ]]\n",
        "W[0]:  [[-0.80474789  0.49508098 -0.05576073 -0.78804255]\n",
        " [ 0.91193213 -0.35282318 -0.94794037  0.6294709 ]\n",
        " [ 0.23154066 -0.92285864  0.71218972  0.04426263]]\n",
        "W[1]:  [[-0.2903014 ]\n",
        " [-0.61821175]\n",
        " [ 0.57824378]\n",
        " [-0.13135747]]\n",
        "W[0]:  [[-0.80546952  0.49285387 -0.05842729 -0.78816943]\n",
        " [ 0.91135354 -0.35460883 -0.95007837  0.62936917]\n",
        " [ 0.23081602 -0.92509501  0.70951206  0.04413522]]\n",
        "W[1]:  [[-0.27604761]\n",
        " [-0.5964872 ]\n",
        " [ 0.55195566]\n",
        " [-0.1258359 ]]\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nn.W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{0: array([[-0.27477492,  0.51654772, -0.72821548, -0.22912741],\n",
        "       [-0.8390646 , -0.37222114, -0.85562827,  0.47701637],\n",
        "       [ 0.86595084, -0.78041616,  0.55266102, -0.19832909]]), 1: array([[-0.32679964, -0.27941225,  0.21816805, -0.25654572],\n",
        "       [-0.43408783,  0.04535826,  0.52730094,  0.25480357],\n",
        "       [ 0.12207412,  0.25902838, -0.20057737,  0.10873974],\n",
        "       [ 0.16722695, -0.41173243,  0.3433458 , -0.40437885]])}\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "array([[ 0.8072749 ],\n",
        "       [ 0.949503  ],\n",
        "       [ 0.23078196],\n",
        "       [ 0.75326884],\n",
        "       [ 0.55774867],\n",
        "       [ 0.64056227],\n",
        "       [ 0.13510974],\n",
        "       [ 0.96724493],\n",
        "       [ 0.13748463],\n",
        "       [ 0.26964399]])"
       ]
      }
     ],
     "prompt_number": 63
    }
   ],
   "metadata": {}
  }
 ]
}